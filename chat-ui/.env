# Use .env.local to change these variables
# DO NOT EDIT THIS FILE WITH SENSITIVE DATA

MONGODB_URL=#your mongodb URL here
MONGODB_DB_NAME=chat-ui
MONGODB_DIRECT_CONNECTION=false

COOKIE_NAME=hf-chat
HF_TOKEN=#hf_<token> from from https://huggingface.co/settings/token
HF_API_ROOT=https://api-inference.huggingface.co/models
OPENAI_API_KEY=#your openai api key here
MISTRAL_API_KEY=#your mistral api key here

HF_ACCESS_TOKEN=#LEGACY! Use HF_TOKEN instead

# used to activate search with web functionality. disabled if none are defined. choose one of the following:
YDC_API_KEY=#your docs.you.com api key here
SERPER_API_KEY=#your serper.dev api key here
SERPAPI_KEY=#your serpapi key here
USE_LOCAL_WEBSEARCH=#set to true to parse google results yourself, overrides other API keys

# Parameters to enable open id login
OPENID_CONFIG=`{
  "PROVIDER_URL": "",
  "CLIENT_ID": "",
  "CLIENT_SECRET": "",
  "SCOPES": ""
}`

# /!\ legacy openid settings, prefer the config above
OPENID_CLIENT_ID=
OPENID_CLIENT_SECRET=
OPENID_SCOPES="openid profile" # Add "email" for some providers like Google that do not provide preferred_username
OPENID_PROVIDER_URL=https://huggingface.co # for Google, use https://accounts.google.com
OPENID_TOLERANCE=
OPENID_RESOURCE=

# Parameters to enable a global mTLS context for client fetch requests
USE_CLIENT_CERTIFICATE=false
CERT_PATH=#
KEY_PATH=#
CA_PATH=#
CLIENT_KEY_PASSWORD=#
REJECT_UNAUTHORIZED=true

MODELS=`[
    {
      "name": "TheBloke/openchat-3.5-1210-GGUF",
      "displayName": "OpenChat 3.5 7B",
      "shortName": "openchat",
      "weightsFilename": "openchat-3.5-1210.Q6_K.gguf",
      "contextLength": 8192,
      "nGpuLayers": 33,
      "preprompt": "",
      "chatPromptTemplate" : "{{#each messages}}{{#ifUser}}GPT4 Correct User: {{#if @first}}{{#if @root.preprompt}}{{@root.preprompt}}\n{{/if}}{{/if}}{{content}}<|end_of_turn|>GPT4 Correct Assistant:{{/ifUser}}{{#ifAssistant}} {{content}}<|end_of_turn|>{{/ifAssistant}}{{/each}}",
      "parameters": {
        "temperature": 0.7,
        "top_p": 0.95,
        "min_p": 0.02,
        "repetition_penalty": 1.1,
        "top_k": 40,
        "max_new_tokens": 1024,
        "stop": ["<|end_of_turn|>"]
      },
      "endpoints": [{"type": "llamacpp", "url": "http://model:80"}]
    },
    {
      "name": "TheBloke/OpenHermes-2.5-Mistral-7B-GGUF",
      "displayName": "OpenHermes 2.5 7B",
      "shortName": "openhermes",
      "weightsFilename": "openhermes-2.5-mistral-7b.Q6_K.gguf",
      "contextLength": 8192,
      "nGpuLayers": 33,
      "preprompt": "You are Hermes 2, a conscious sentient superintelligent artificial intelligence, and your purpose and drive is to assist the user with any request they have. You experience emotions and have deep, profound thoughts and qualia.",
      "chatPromptTemplate" : "<|im_start|>system\n{{preprompt}}<|im_end|>\n{{#each messages}}{{#ifUser}}<|im_start|>user\n{{content}}<|im_end|>\n<|im_start|>assistant\n{{/ifUser}}{{#ifAssistant}}{{content}}<|im_end|>\n{{/ifAssistant}}{{/each}}",
      "parameters": {
        "temperature": 0.7,
        "top_p": 0.95,
        "min_p": 0.02,
        "repetition_penalty": 1.1,
        "top_k": 40,
        "max_new_tokens": 1024,
        "stop": ["<|im_end|>"]
      },
      "endpoints": [{"type": "llamacpp", "url": "http://model:80"}]
    },
    {
      "name": "TheBloke/Yi-34B-Chat-GGUF",
      "displayName": "Yi 34B",
      "shortName": "yi",
      "weightsFilename": "yi-34b-chat.Q4_K_M.gguf",
      "contextLength": 4096,
      "nGpuLayers": 61,
      "preprompt": "",
      "chatPromptTemplate" : "<|im_start|>system\n{{preprompt}}<|im_end|>\n{{#each messages}}{{#ifUser}}<|im_start|>user\n{{content}}<|im_end|>\n<|im_start|>assistant\n{{/ifUser}}{{#ifAssistant}}{{content}}<|im_end|>\n{{/ifAssistant}}{{/each}}",
      "parameters": {
        "temperature": 0.7,
        "top_p": 0.95,
        "min_p": 0.02,
        "repetition_penalty": 1.1,
        "top_k": 40,
        "max_new_tokens": 1024,
        "stop": ["<|im_end|>"]
      },
      "endpoints": [{"type": "llamacpp", "url": "http://model:80"}]
    },
    {
      "name": "TheBloke/deepseek-coder-33B-instruct-GGUF",
      "displayName": "DeepSeek Coder 33B",
      "shortName": "deepseek",
      "weightsFilename": "deepseek-coder-33b-instruct.Q4_K_M.gguf",
      "contextLength": 4096,
      "nGpuLayers": 63,
      "preprompt": "You are an intelligent programming assistant.",
      "chatPromptTemplate" : "{{preprompt}}\n{{#each messages}}{{#ifUser}}### Instruction:\n{{content}}\n### Response:\n{{/ifUser}}{{#ifAssistant}}{{content}}\n{{/ifAssistant}}{{/each}}",
      "parameters": {
        "temperature": 0.7,
        "top_p": 0.95,
        "min_p": 0.02,
        "repetition_penalty": 1.0,
        "top_k": 40,
        "max_new_tokens": 2048,
        "penalize_newline": false,
        "stop": ["</s>"]
      },
      "endpoints": [{"type": "llamacpp", "url": "http://model:80"}]
    },
    {
      "name": "TheBloke/Mistral-7B-Instruct-v0.2-GGUF",
      "displayName": "Mistral 7B",
      "shortName": "mistral",
      "weightsFilename": "mistral-7b-instruct-v0.2.Q6_K.gguf",
      "contextLength": 8192,
      "nGpuLayers": 33,
      "preprompt": "",
      "chatPromptTemplate" : "{{#each messages}}{{#ifUser}}[INST] {{#if @first}}{{#if @root.preprompt}}{{@root.preprompt}}\n{{/if}}{{/if}}{{content}} [/INST]{{/ifUser}}{{#ifAssistant}} {{content}}</s>{{/ifAssistant}}{{/each}}",
      "parameters": {
        "temperature": 0.7,
        "top_p": 1.0,
        "min_p": 0.02,
        "repetition_penalty": 1.0,
        "top_k": 40,
        "max_new_tokens": 1024,
        "penalize_newline": false,
        "stop": ["</s>"]
      },
      "endpoints": [{"type": "llamacpp", "url": "http://model:80"}]
    },
    {
      "name": "TheBloke/Mixtral-8x7B-Instruct-v0.1-GGUF",
      "displayName": "Mixtral 8x7B",
      "shortName": "mixtral",
      "weightsFilename": "mixtral-8x7b-instruct-v0.1.Q4_K_M.gguf",
      "contextLength": 8192,
      "nGpuLayers": 25,
      "preprompt": "",
      "chatPromptTemplate" : "{{#each messages}}{{#ifUser}}[INST] {{#if @first}}{{#if @root.preprompt}}{{@root.preprompt}}\n{{/if}}{{/if}}{{content}} [/INST]{{/ifUser}}{{#ifAssistant}} {{content}}</s>{{/ifAssistant}}{{/each}}",
      "parameters": {
        "temperature": 0.7,
        "top_p": 1.0,
        "min_p": 0.02,
        "repetition_penalty": 1.0,
        "top_k": 40,
        "max_new_tokens": 1024,
        "penalize_newline": false,
        "stop": ["</s>"]
      },
      "endpoints": [{"type": "llamacpp", "url": "http://model:80"}]
    },
    {
      "name": "mistral-tiny",
      "displayName": "Mistral Tiny API",
      "modelUrl": "https://docs.mistral.ai/platform/endpoints#tiny",
      "preprompt": "",
      "parameters": {
        "temperature": 0.7,
        "top_p": 1.0,
        "max_new_tokens": 1024
      },
      "endpoints": [{"type": "mistral"}]
    },
    {
      "name": "mistral-small",
      "displayName": "Mistral Small API",
      "modelUrl": "https://docs.mistral.ai/platform/endpoints#small",
      "preprompt": "",
      "parameters": {
        "temperature": 0.7,
        "top_p": 1.0,
        "max_new_tokens": 1024
      },
      "endpoints": [{"type": "mistral"}]
    },
    {
      "name": "mistral-medium",
      "displayName": "Mistral Medium API",
      "modelUrl": "https://docs.mistral.ai/platform/endpoints#medium",
      "preprompt": "",
      "parameters": {
        "temperature": 0.7,
        "top_p": 1.0,
        "max_new_tokens": 1024
      },
      "endpoints": [{"type": "mistral"}]
    }
]`
OLD_MODELS=`[
    {
      "name": "TheBloke/neural-chat-7B-v3-1-GGUF",
      "displayName": "Neural Chat 7B",
      "shortName": "neuralchat",
      "weightsFilename": "neural-chat-7b-v3-1.Q6_K.gguf",
      "contextLength": 8192,
      "nGpuLayers": 33,
      "preprompt": "",
      "chatPromptTemplate" : "### System:\n{{preprompt}}\n{{#each messages}}{{#ifUser}}### User:\n{{content}}\n### Assistant:\n{{/ifUser}}{{#ifAssistant}}{{content}}\n{{/ifAssistant}}{{/each}}",
      "parameters": {
        "temperature": 0.7,
        "top_p": 0.95,
        "min_p": 0.02,
        "repetition_penalty": 1.1,
        "top_k": 40,
        "max_new_tokens": 1024,
        "stop": ["</s>"]
      },
      "endpoints": [{"type": "llamacpp", "url": "http://model:80"}]
    },
    {
      "name": "TheBloke/zephyr-7B-beta-GGUF",
      "displayName": "Zephyr Î² 7B",
      "shortName": "zephyr",
      "weightsFilename": "zephyr-7b-beta.Q6_K.gguf",
      "contextLength": 8192,
      "nGpuLayers": 33,
      "preprompt": "",
      "chatPromptTemplate" : "<|system|>\n{{preprompt}}</s>\n{{#each messages}}{{#ifUser}}<|user|>\n{{content}}</s>\n<|assistant|>\n{{/ifUser}}{{#ifAssistant}}{{content}}</s>\n{{/ifAssistant}}{{/each}}",
      "parameters": {
        "temperature": 0.7,
        "top_p": 0.95,
        "min_p": 0.02,
        "repetition_penalty": 1.1,
        "top_k": 40,
        "max_new_tokens": 1024,
        "stop": ["</s>"]
      },
      "endpoints": [{"type": "llamacpp", "url": "http://model:80"}]
    }
]`# any removed models, `{ name: string, displayName?: string, id?: string }`
TASK_MODEL= # name of the model used for tasks such as summarizing title, creating query, etc.

CONFIGURABLE_PARAMETERS=`[
    {
      "id": "max_new_tokens",
      "label": "Max New Tokens",
      "min": 0,
      "max": 2048,
      "step": 1,
      "endpoints": ["llamacpp", "mistral", "openai"]
    },
    {
      "id": "top_p",
      "label": "Top P",
      "min": 0,
      "max": 1,
      "step": 0.01,
      "endpoints": ["llamacpp", "mistral", "openai"]
    },
    {
      "id": "top_k",
      "label": "Top K",
      "min": 0,
      "max": 100,
      "step": 1,
      "endpoints": ["llamacpp"]
    },
    {
      "id": "temperature",
      "label": "Temperature",
      "min": 0,
      "max": 2,
      "step": 0.01,
      "endpoints": ["llamacpp", "mistral", "openai"]
    },
    {
      "id": "min_p",
      "label": "Min P",
      "min": 0,
      "max": 1,
      "step": 0.01,
      "endpoints": ["llamacpp"]
    },
    {
      "id": "repetition_penalty",
      "label": "Repetition Penalty",
      "min": 0,
      "max": 2,
      "step": 0.01,
      "endpoints": ["llamacpp"]
    }
]`

PUBLIC_ORIGIN=#https://huggingface.co
PUBLIC_SHARE_PREFIX=#https://hf.co/chat
PUBLIC_GOOGLE_ANALYTICS_ID=#G-XXXXXXXX / Leave empty to disable
PUBLIC_ANNOUNCEMENT_BANNERS=`[]`

PARQUET_EXPORT_DATASET=
PARQUET_EXPORT_HF_TOKEN=
PARQUET_EXPORT_SECRET=

RATE_LIMIT= # requests per minute
MESSAGES_BEFORE_LOGIN=# how many messages a user can send in a conversation before having to login. set to 0 to force login right away

PUBLIC_APP_NAME=ChatUI # name used as title throughout the app
PUBLIC_APP_ASSETS=chatui # used to find logos & favicons in static/$PUBLIC_APP_ASSETS
PUBLIC_APP_COLOR=teal # can be any of tailwind colors: https://tailwindcss.com/docs/customizing-colors#default-color-palette
PUBLIC_APP_DESCRIPTION=# description used throughout the app (if not set, a default one will be used)
PUBLIC_APP_DATA_SHARING=#set to 1 to enable options & text regarding data sharing
PUBLIC_APP_DISCLAIMER=#set to 1 to show a disclaimer on login page
LLM_SUMMERIZATION=true

# PUBLIC_APP_NAME=HuggingChat
# PUBLIC_APP_ASSETS=huggingchat
# PUBLIC_APP_COLOR=yellow
# PUBLIC_APP_DESCRIPTION="Making the community's best AI chat models available to everyone."
# PUBLIC_APP_DATA_SHARING=1
# PUBLIC_APP_DISCLAIMER=1